<!doctype html>
<meta charset="utf-8">
<style>
body {
  overflow-x: hidden;
}
.scroll-down {
  width: 80px;
  height: 40px;
  right: 10px;
  bottom: 10px;
  position: absolute;
  font-family: "Roboto","Helvetica Neue",Helvetica,Arial,sans-serif;
  font-size: 12px;
  font-weight: 300;
  color: #FFFFFF;
  opacity: 0;
  -webkit-transition: opacity 2s ease-in;
  -moz-transition: opacity 2s ease-in;
  -o-transition: opacity 2s ease-in;
  -ms-transition: opacity 2s ease-in;
  transition: opacity 2s ease-in;
}
.scroll-down span {
  margin-top: 5px;
  position: absolute;
  left: 50%;
  transform: translate(-100%, 0) rotate(45deg);
  transform-origin: 100% 100%;
  height: 2px;
  width: 10px;
  background: #FFFFFF;
}
.scroll-down span:nth-of-type(2) {
  transform-origin: 0 100%;
  transform: translate(0, 0) rotate(-45deg);
}
.spinner {
  position: absolute;
  height: 160px;
  width: 160px;
  -webkit-animation: rotation .6s infinite linear;
  -moz-animation: rotation .6s infinite linear;
  -o-animation: rotation .6s infinite linear;
  animation: rotation .6s infinite linear;
  border-left: 6px solid rgba(0, 174, 239, .15);
  border-right: 6px solid rgba(0, 174, 239, .15);
  border-bottom: 6px solid rgba(0, 174, 239, .15);
  border-top: 6px solid rgba(0, 174, 239, .8);
  border-radius: 100%;
  top: calc(50% - 100px);
  left: calc(50% - 80px);
  right: auto;
  bottom: auto;
}

@-webkit-keyframes rotation {
  from {
    -webkit-transform: rotate(0deg);
  }
  to {
    -webkit-transform: rotate(359deg);
  }
}
.transparent {
  opacity: 0;
}

figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

dt-article figcaption b {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

*.unselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
}
*.svgunselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
    background: none;
    pointer-events: none;
}
</style>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <!-- roboto font -->
  <!--<link href='https://fonts.googleapis.com/css?family=Roboto:300' rel='stylesheet' type='text/css'>-->

  <meta name="theme-color" content="#ffffff" />

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-S9HB5HYREL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-S9HB5HYREL');
  </script>

  <!-- SEO -->
  <meta property="og:title" content="SayTap: Language to Quadrupedal Locomotion" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="Use foot contact pattern to bridge LLM and low-level controller." />
  <meta property="og:image" content="https://saytap.github.io/assets/card/saytap_rect.png" />
  <meta property="og:url" content="https://saytap.github.io/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="SayTap: Language to Quadrupedal Locomotion" />
  <meta name="twitter:description" content="Use foot contact pattern to bridge LLM and low-level controller." />
  <meta property="og:site_name" content="SayTap: Language to Quadrupedal Locomotion" />
  <meta name="twitter:image" content="https://saytap.github.io/assets/card/saytap_square.png" />
</head>
<link rel="stylesheet" href="css/katex.min.css">

<!--<script src="lib/jquery-1.12.4.min.js"></script>
<script src="lib/mobile-detect.min.js"></script>-->
<script src="lib/template.v1.js"></script>

<script type="text/front-matter">
  title: "SayTap: Language to Quadrupedal Locomotion"
  description: ""
</script>
<body>
  <div id="no_javasript_warning">
    <h3>This page requires Javascript. Please enable it for <code>https://saytap.github.io/</code></h3>
  </div>
  <script>
    document.getElementById("no_javasript_warning").style.display = "none";
  </script>
<div style="text-align: center;">
  <video src="assets/mp4/banner.mp4", type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;"></video>
</div>

<dt-article id="dtbody">

  <div style="text-align: center;">
    <figcaption style="text-align: left; color:#FF6C00; padding-top: 0;"><br/>Examples of A1 following human commands in natural language</figcaption>
    <figcaption style="text-align: left; padding-top: 0;">
    Unlike text generation where Large Language Models (LLMs) directly interpret the atomic elements‚Äîtokens‚Äîit often proves challenging for LLMs to comprehend low-level robotic
    commands such as joint angle targets or motor torques, especially for inherently unstable legged robots necessitating high-frequency control signals.
    In this work, we propose to use foot contact patterns as an interface that bridges human instructions in natural language and low-level commands.
    The proposed approach allows the robot to take both simple and direct instructions (e.g., ‚ÄúTrot forward slowly‚Äù) as well as vague human commands (e.g., ‚ÄúGood news, we are going to
    a picnic this weekend!‚Äù) in natural language and react accordingly..<br/>
    </figcaption>
  </div>

<!--
<h2>‚ö†Ô∏è&nbsp;Draft. Please do not share this article‚ùóÔ∏èüôèüèºüôáüèª</h2>
-->

<dt-byline class="l-page transparent"></dt-byline>
<h1>SayTap: Language to Quadrupedal Locomotion</h1>
<p></p>
<dt-byline class="l-page" id="authors_section" hidden>
  <div class="authors">
    <div class="author">
        <a class="name" href="https://twitter.com/yujin_tang">Yujin Tang</a>
        <a class="affiliation" href="https://research.google/teams/brain/">Google DeepMind</a>
        <a class="affiliation" href="https://www.u-tokyo.ac.jp/en/">The University of Tokyo</a>
    </div>
    <div class="author">
        <a class="name" href="https://wenhaoyu.weebly.com/">Wenhao Yu</a>
        <a class="affiliation" href="https://research.google/teams/brain/">Google DeepMind</a>
        &nbsp;
    </div>
    <div class="author">
      <a class="name" href="https://www.jie-tan.net/">Jie Tan</a>
      <a class="affiliation" href="https://research.google/teams/brain/">Google DeepMind</a>
      &nbsp;
    </div>
    <div class="author">
      <a class="name" href="https://twitter.com/heiga_zen">Heiga Zen</a>
      <a class="affiliation" href="https://research.google/teams/brain/">Google DeepMind</a>
      &nbsp;
    </div>
    <div class="author">
      <a class="name" href="https://twitter.com/AleksandraFaust">Aleksandra Faust</a>
      <a class="affiliation" href="https://research.google/teams/brain/">Google DeepMind</a>
      &nbsp;
    </div>
    <div class="author">
        <a class="name" href="https://twitter.com/ttyharada">Tatsuya Harada</a>
        <a class="affiliation" href="https://www.u-tokyo.ac.jp/en/">The University of Tokyo</a>
        &nbsp;
    </div>
  </div>
  <div class="date">
    <div class="month">Jun 14</div>
    <div class="year">2023</div>
    &nbsp;
  </div>
  <div class="date">
    <div class="month">Arxiv</div>
    <div class="year" style="color: #FF6C00;"><a href="https://arxiv.org/abs/2306.07580" target="_blank">paper</a></div>
    &nbsp;
  </div>
</dt-byline>
<!--## Abstract-->
<p>Large language models (LLMs) have demonstrated the potential to
perform high-level planning. Yet, it remains a challenge for LLMs to comprehend
low-level commands, such as joint angle targets or motor torques. This work
proposes an approach to use foot contact patterns as an interface that bridges
human commands in natural language and a locomotion controller that outputs
these low-level commands. This results in an interactive system for quadrupedal
robots that allows the users to craft diverse locomotion behaviors flexibly. We
contribute an LLM prompt design, a reward function, and a method to expose the
controller to the feasible distribution of contact patterns. The results are a controller
capable of achieving diverse locomotion patterns that can be transferred to real
robot hardware. Compared with other design choices, the proposed approach
enjoys more than 50% success rate in predicting the correct contact patterns and
can solve 10 more tasks out of a total of 30 tasks.</p>
<h2>Method</h2>
<p>The core ideas of our approach include introducing desired foot contact patterns as a new interface between human commands in natural language and the locomotion controller. The locomotion controller is required to not only complete the main task (e.g., following specified velocities), but also to place the robot's feet on the ground at the right time, such that the realized foot contact patterns are as close as possible to the desired ones, the following figure gives an overview of the proposed system. To achieve this, the locomotion controller takes a desired foot contact pattern at each time step as its input, in addition to the robot's proprioceptive sensory data and task related inputs (e.g., user specified velocity commands). At training, a random generator creates these desired foot contact patterns, while at test time a LLM translates them from human commands.</p>
<p>In this paper, a desired foot contact pattern is defined by a cyclic sliding window of size <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">L_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> that extracts the four feet ground contact flags between <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mo>+</mo><msub><mi>L</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">t+L_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span> from a pattern template and is of shape <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mo>√ó</mo><msub><mi>L</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">4\times L_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">4</span><span class="mbin">√ó</span><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">‚Äã</span></span>‚Äã</span></span></span></span></span></span>. A contact pattern template is a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mo>√ó</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">4 \times T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">4</span><span class="mbin">√ó</span><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span></span> matrix of '0's and '1's, with '0's representing feet in the air and '1's for feet on the ground. From top to bottom, each row in the matrix gives the foot contact patterns of the front left (FL), front right (FR), rear left (RL) and rear right (RR) feet. We demonstrate that the LLM is capable of mapping human commands into foot contact pattern templates in specified formats accurately given properly designed prompts, even in cases when the commands are unstructured and vague. In training, we use a random pattern generator to produce contact pattern templates that are of various pattern lengths <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span></span>, foot-ground contact ratios within a cycle based on a given gait type <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">G</span></span></span></span>, so that the locomotion controller gets to learn on a wide distribution of movements and generalizes better.</p>
<p>Please check out our paper for more details.</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/jpeg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/overview.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption>
Overview of the proposed approach. In addition to the robot's proprioceptive sensory data and task commands (e.g., following a desired linear velocity), the locomotion controller accepts desired foot contact patterns as input, and outputs desired joint positions. The foot contact patterns are extracted by a cyclic sliding window from a pattern template, which is generated by a random pattern generator during training, and is translated from human commands in natural language by an LLM in tests. We show some examples of contact pattern templates at the bottom.<br/>
</figcaption>
</div>
<h2>Videos</h2>
<h3>Following Simple/Direct Commands</h3>
<div style="text-align: center;">
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/basic_test1.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/basic_test2.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
</table>
<figcaption style="text-align: left; padding-top: 0;">
Videos that show our A1 following simple/direct instructions.<br/>
</figcaption>
</div>
<h3>Following Unstructured/Vague Commands</h3>
<div style="text-align: center;">
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/extended_test1.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/extended_test2.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/extended_test3.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/extended_test4.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/extended_test5.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
</table>
<figcaption style="text-align: left; padding-top: 0;">
Videos that show our A1 following unstructured/vague commands.<br/>
</figcaption>
</div>
<!--
______

## Introduction

Simple and effective interaction between human and quadrupedal robots paves the way towards creating intelligent and capable helper robots, forging a future where technology enhances our lives in ways beyond our imagination <dt-cite key="borenstein1997guidecane,chuang2018deep,mehrizi2021quadrupedal"></dt-cite>. Key to such human-robot interaction system is enabling quadrupedal robots to respond to natural language instructions as language is one of the most important communication channels for human beings.
Recent developments in Large Language Models (LLMs) have engendered a spectrum of applications that were once considered unachievable, including virtual assistance <dt-cite key="NEURIPS2022_b1efde53"></dt-cite>, code generation <dt-cite key="chen2021evaluating"></dt-cite>, translation <dt-cite key="chowdhery2022palm"></dt-cite>, and logical reasoning <dt-cite key="wei2022chain"></dt-cite>, fueled by the proficiency of LLMs to ingest an enormous amount of historical data, to adapt in-context to novel tasks with few examples, and to understand and interact with user intentions through a natural language interface.

The burgeoning success of LLMs has also kindled interest within the robotics researcher community, with an aim to develop interactive and capable systems for physical robots <dt-cite key="ahn2022can,huang2022inner,lynch2022interactive,liang2022code,singh2022progprompt,vemprala2023chatgpt"></dt-cite>. Researchers have demonstrated the potential of using LLMs to perform high-level planning <dt-cite key="ahn2022can,huang2022inner"></dt-cite>, and robot code writing <dt-cite key="liang2022code,vemprala2023chatgpt"></dt-cite>. Nevertheless, unlike text generation where LLMs directly interpret the atomic elements‚Äîtokens‚Äîit often proves challenging for LLMs to comprehend low-level robotic commands such as joint angle targets or motor torques, especially for inherently unstable legged robots necessitating high-frequency control signals. Consequently, most existing work presume the provision of high-level APIs for LLMs to dictate robot behaviour, inherently limiting the system's expressive capabilities.

We address this limitation by using foot contact patterns as an interface that bridges human instructions in natural language and low-level commands. The result is an interactive system for legged robots, particularly quadrupedal robots, that allows users to craft diverse locomotion behaviours flexibly. Central to the proposed approach is the observation that patterns of feet establishing and breaking contacts with the ground often govern the final locomotion behavior for legged robots due to the heavy reliance of quadruped locomotion on environmental contact. Thus, a contact pattern, describing the contact establishing and breaking timings for each legs, is a compact and flexible interface to author locomotion behaviors for legged robots. To leverage this new interface for controlling quadruped robots, we first develop an LLM-based approach to generate contact patterns, represented by '0's and '1's, from user instructions. Despite that LLMs are trained with mostly natural language dataset, we find that with proper prompting and in-context learning, they can produce contact patterns to represent diverse quadruped motions. We then develop a Deep Reinforcement Learning (DRL) based approach to generate robot actions given a desired contact pattern. We demonstrate that by designing a reward structure that only concerns about contact timing and exposing the policy to the right distribution of contact patterns, we can obtain a controller capable of achieving diverse locomotion patterns that can be transferred to the real robot hardware.

<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/cover_image_v3.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption>
Illustration of the results on a physical quadrupedal robot. We show two test commands at the top, and the snapshots of the robot in the top row of the figure. The row in the middle shows the desired contact patterns translated from the commands by an LLM, and the bottom row gives the realized patterns. The proposed approach allows the robot to take both simple and direct instructions (e.g., "Trot forward slowly") as well as vague human commands (e.g., "Good news, we are going to a picnic this weekend!") in natural language and react accordingly.<br/>
</figcaption>
</div>

We evaluate the proposed approach on a physical quadruped robot, Unitree A1, where it successfully controls the robot to follow diverse and challenging instructions from users. We benchmark the proposed approach against two baselines: (1) using discrete gaits, and (2) using sinusoidal functions as interface.  Evaluations on $30$ tasks demonstrate that the proposed approach can achieve $50\%$ higher success rate in predicting the correct contact pattern and can solve $10$ more tasks than the baselines.

The key contributions of this paper are: i) A novel interface of contact pattern for harnessing knowledge from LLMs to flexibly and interactive control quadruped robots; ii) A pipeline to teach LLMs to generate complex contact patterns from user instructions; iii) A DRL-based method to train a low-level controller that realizes diverse contact patterns on real quadruped robots. Finally, our proposal also holds intriguing potential for both human-robot interaction researchers and the robotic locomotion community, inviting a compelling cross-disciplinary dialogue and collaboration.

______

## Related Works

### Language to Robot Control

There is a rich literature in leveraging language to modulate the behavior of robots <dt-cite key="tellex2020robots,lynch2022interactive,ahn2022can,zeng2022socratic,kress2008translating,stepputtis2020language,chai2018language,howard2014natural"></dt-cite>. Earlier work in this direction typically assumes structured text templates to translate language to robot commands <dt-cite key="kress2008translating,chai2018language"></dt-cite> or leveraged natural language processing (NLP) tools such as the parse tree to assist extracting the constraints from user input, followed by trajectory optimization to obtain robot motion <dt-cite key="howard2014natural"></dt-cite>. Though these approaches demonstrate complex robotics tasks, they usually do not handle unstructured natural language input. To mitigate this issue, recent work leverages the advancements in representation learning and deep learning to train language conditioned policies that mapped unstructured natural language instructions to robot actions <dt-cite key="stepputtis2020language,brohan2022rt,mees2022matters,shridhar2022cliport"></dt-cite>. To establish proper mappings between natural language embeddings and robot actions, these approaches usually require a significant amount of demonstration data with language labels for training the policy, which is challenging to collect for diverse legged locomotion behaviors.

Inspired by recent success in LLMs to perform diverse tasks <dt-cite key="chen2021evaluating,chowdhery2022palm,wei2022chain"></dt-cite>, researchers in robotics have also explored ideas to connect LLMs to robot commands <dt-cite key="ahn2022can,huang2022inner,liang2022code,singh2022progprompt,vemprala2023chatgpt,lin2023text2motion,bucker2022latte"></dt-cite>. For example, <dt-cite key="ahn2022can"></dt-cite> combined LLMs with a learned robot affordance function to pick the optimal pre-trained robot skills for completing long horizon tasks. To mitigate the requirement for pre-training individual low-level skills, researchers also proposed to expand the low-level primitive skills to the full expressiveness of code by tasking LLMs to write robot codes <dt-cite key="liang2022code,singh2022progprompt,vemprala2023chatgpt"></dt-cite>. As LLMs cannot directly generate low-level robot motor commands such as joint targets, these approaches had to design an intermediate interface for connecting LLMs and robot commands, such as high-level plans <dt-cite key="ahn2022can,huang2022inner,lin2023text2motion"></dt-cite>, primitive skills <dt-cite key="liang2022code,singh2022progprompt"></dt-cite>, and trajectories <dt-cite key="bucker2022latte"></dt-cite>.
In this work, we identify foot contact patterns to be a natural and flexible intermediate interface for quadrupedal robot locomotion that do not require laborious design efforts.

### Locomotion Controller for Legged Robots

Training legged robots to exhibit complex contact patterns, especially gait patterns, has been extensively studied by researchers in robotics, control, and machine learning. A common method is to model the robot dynamics and perform receding horizon trajectory optimization, i.e., Model Predictive Control (MPC), to follow desired contact patterns <dt-cite key="di2018dynamic,grandia2019feedback,li2022versatile,villarreal2020mpc,winkler18"></dt-cite>. For quadruped robots, this led to a large variety of canonical locomotion gaits such as trotting <dt-cite key="di2018dynamic"></dt-cite>, pacing <dt-cite key="raibert1990trotting"></dt-cite>, bounding <dt-cite key="eckert2015comparing"></dt-cite>, and galloping <dt-cite key="marhefka2003intelligent"></dt-cite>, as well as non conventional gaits specified by the desired contact timing or patterns <dt-cite key="li2022versatile,winkler18"></dt-cite>. Despite the impressive results in these work, applying MPC to generate diverse locomotion behavior often requires careful design of reference motion for robot base and swing legs and high computational cost due to re-planning. Prior work have also explored using learning-based methods to author flexible locomotion gaits <dt-cite key="yang2020data,margolis2021learning,da2021learning,siekmann2021sim,iscen2018policies,hwangbo2019learning"></dt-cite>. Some of these work combines learning and MPC-based methods to identify the optimal gait parameters for tasks <dt-cite key="yang2020data,margolis2021learning,da2021learning"></dt-cite>. Others directly train DRL policies for different locomotion gaits, either through careful reward function design <dt-cite key="siekmann2021sim"></dt-cite>, open-loop commands extracted from prior knowledge about gaits <dt-cite key="iscen2018policies,hwangbo2019learning"></dt-cite> or encoding of a predefined family of locomotion strategies that solve training tasks in different ways <dt-cite key="margolis2023walk"></dt-cite>. This paper explores an alternative DRL-based method that relies on the simple but flexible reward structure. Compared to the prior work, the proposed reward structure only concerns about contact timing thus is more flexible in generating diverse locomotion behaviors on the robot.


______

## Method

The core ideas of our approach include introducing desired foot contact patterns as a new interface between human commands in natural language and the locomotion controller. The locomotion controller is required to not only complete the main task (e.g., following specified velocities), but also to place the robot's feet on the ground at the right time, such that the realized foot contact patterns are as close as possible to the desired ones, the following figure gives an overview of the proposed system. To achieve this, the locomotion controller takes a desired foot contact pattern at each time step as its input, in addition to the robot's proprioceptive sensory data and task related inputs (e.g., user specified velocity commands). At training, a random generator creates these desired foot contact patterns, while at test time a LLM translates them from human commands.

In this paper, a desired foot contact pattern is defined by a cyclic sliding window of size $L_w$ that extracts the four feet ground contact flags between $t+1$ and $t+L_w$ from a pattern template and is of shape $4\times L_w$. A contact pattern template is a $4 \times T$ matrix of '0's and '1's, with '0's representing feet in the air and '1's for feet on the ground. From top to bottom, each row in the matrix gives the foot contact patterns of the front left (FL), front right (FR), rear left (RL) and rear right (RR) feet. We demonstrate that the LLM is capable of mapping human commands into foot contact pattern templates in specified formats accurately given properly designed prompts, even in cases when the commands are unstructured and vague. In training, we use a random pattern generator to produce contact pattern templates that are of various pattern lengths $T$, foot-ground contact ratios within a cycle based on a given gait type $G$, so that the locomotion controller gets to learn on a wide distribution of movements and generalizes better.

<div style="text-align: center;">
<img class="b-lazy" src=data:image/jpeg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/overview.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption>
Overview of the proposed approach. In addition to the robot's proprioceptive sensory data and task commands (e.g., following a desired linear velocity), the locomotion controller accepts desired foot contact patterns as input, and outputs desired joint positions. The foot contact patterns are extracted by a cyclic sliding window from a pattern template, which is generated by a random pattern generator during training, and is translated from human commands in natural language by an LLM in tests. We show some examples of contact pattern templates at the bottom.<br/>
</figcaption>
</div>


### Language to Foot Contact Patterns

Although LLMs can learn knowledge from a vast amount of text data at training, providing proper prompts at inference is the key to unlock and direct the acquired knowledge in meaningful ways. Carefully designed prompts serve as the starting point for the models to generate text and guide the direction and context of the outputs.  The proposed approach aims to enable the LLM to map any human commands in natural language to foot contact patterns in a specified format. The following figure lists the prompts used in this paper, wherein we group them into four categories:

- **General instruction** describes the task the LLM should accomplish. In this paper, the LLM is expected to translate an arbitrary command to a foot contact pattern.  Note that examples of such translations will be provided in **Examples block**.
- **Gait definition** gives basic knowledge of quadrupedal gaits. Although their descriptions are neither exhaustive nor sufficiently accurate, experimental results suggest that it provides enough information for the LLM to follow the rules.  It also connects the bounding gait to a general impression of emotion.  This helps the LLM generalize over vague human commands that do not explicitly specify what gaits the robot should use.
- **Output format definition** specifies the format of the output. We discretize the desired velocities $\hat{v}_x \in \{-1, -0.5, 0, 0.5, 1\}\frac{m}{s}$ so that the LLM can give proper outputs corresponding to commands that contain words like "fast(er)" and "slow(er)".
- **Examples block** follows the general knowledge of instruction fine-tuning and shows the LLM a few concrete input-output pairs.  Although we give the LLM three commonly seen gait examples only, experimental results show that it is able to generalize and handle various commands, including those vaguely state what velocity or gait the robot should use.

<div style="text-align: center;">
<a href="assets/png/lm_prompt.png" target="_blank"><img class="b-lazy" src=data:image/jpeg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/lm_prompt.png" style="display: block; margin: auto; width: 100%;"/></a>
<figcaption>
Our prompt. Texts in black are for explanation and are not used as input to the LLM. (Click the image to enlarge)<br/>
</figcaption>
</div>

### Foot Contact Pattern to Low-level Commands

#### Problem Formation

We formulate locomotion control as a Markov Decision Process (MDP) and solve it using DRL algorithms. An MDP is a tuple $(S, A, r, f, P_0, \gamma)$, where $S$ is the state space, $A$ is the action space, $r(s_t, a_t, s_{t+1})$ is the reward function, $f(s_t, a_t)$ is the system transition function, $P_0$ is the distribution of initial states $s_0$, and $\gamma \in [0, 1]$ is the reward discount factor. The goal of a DRL algorithm is to optimize a policy $\pi: S \mapsto A$ so that the expected accumulated reward $J = E_{s_0\sim P_0} [\sum_t \gamma^t r(s_t, a_t, s_{t+1})]$ is maximized. Here, $a_t = \pi_{\theta}(s_t)$ and $\theta$ is the set of learnable parameters. In locomotion tasks, $s_t$ often includes sensory data and goal conditions (e.g., user specified velocity commands <dt-cite key="rudin2022learning"></dt-cite>), and $a_t$ is desired joint angles or motor torques.  We expand $s_t$ to include a desired foot contact pattern, and the controller needs to achieve the main task as well as realize the desired foot contact patterns.

#### Random Pattern Generator

The random pattern generator receives a gait type $G$ and outputs a pattern template correspondingly. While humans can give commands that map to a much wider set of foot contact pattern templates, we define and train on five types: $G \in \{\mathrm{BOUND}, \mathrm{TROT}, \mathrm{PACE}, \mathrm{STAND\_STILL}, \mathrm{STAND\_3LEGS}\}$. Examples of the first three types are illustrated at the bottom of overview figure, the latter two types are trivial and omitted in the figure. To generate a template, the generator needs to sample pattern length $T$ and the ground contact length within the cycle for each feet, then conduct proper scaling and phase shifting. We defer the details in the Appendix.

#### Locomotion Controller

We use a feed-forward neural network as the control policy $\pi_{\theta}$. It outputs the desired positions for each motor joint and its input includes the base's angular velocities, the gravity vector $\vec{g}=[0, 0, -1]$ in the base's frame, user specified velocity, current joint positions and velocities, policy output from the last time step, and desired foot contact patterns. In this paper, we use Unitree A1 as the quadrupedal robot. A1 has 3 joints per leg (i.e., hip, thigh and calf joints) and $L_w=5$ in all experiments, therefore the dimensions of the policy's input and output are $65$ and $12$, respectively.  The policy has three hidden layers of sizes $[512, 256, 128]$ with $\mathrm{ELU}(\alpha=1.0)$ at each hidden layer as the non-linear activation function.

To encourage natural and symmetric behaviors, we employ a double-pass trick in the control policy. Specifically, instead of using $a_t=\pi_{\theta}(s_t)$ directly as the output, we use $a_t=0.5[ \pi_{\theta}(s_t) + f_{\mathrm{act}} \large{(} \pi_{\theta}(f_{\mathrm{obs}}(s_t) \large{)} ]$, where $f_{\mathrm{act}}(\cdot)$ and $f_{\mathrm{obs}}(\cdot)$ flips left-right the policy's output and the robot's state respectively. Intuitively, this double-pass trick says the control policy should output consistently when it receives the original and the left-right mirrored states. In practice, we find this trick greatly improves the naturalness of the robot's movement and helped shrink the sim-to-real gap.


#### Task and Training Setups

The controller's main task is to follow user specified linear velocities along the robot's heading direction, while keeping the linear velocity along the lateral direction and the yaw angular velocity as close to zeros as possible. At the same time, it also needs to plan for the correct timing for feet-ground strikes so that the realized foot contact patterns match the desired ones. For real world deployment, we add a regularization term that penalizes action changing rate so that the real robot's movement is smoother. Finally, in addition to applying domain randomization, we find that extra reward terms that keep the robot base stable can greatly shrink the sim-to-real gap and produce natural looking gaits. Please refer to the Appendix for detailed reward terms, domain randomization configurations and other training related settings.


______


## Experiments

We conducted experiments to answer three questions. 
Throughout the experiments, we used GPT-4 <dt-cite key="openai2023gpt"></dt-cite> as the LLM.
Please see the Appendix for experimental setups.


### Is Foot Contact Pattern a Good Interface?

The first experiment compares foot contact pattern with other possible interface designs. One option is to introduce intermediate parameters as the interface, and have the LLM map from human natural language to the parameter values. We use two baseline approaches for comparison: Baseline 1 contains a discrete parameter $G$ that is the 5 gait types that we introduced earlier; Baseline 2 contains 4 tuples of continuous parameters $(a_i, b_i, c_i), i \in \{1, 2, 3, 4\}$ that defines a sinusoidal function $y_i(t)=\sin(a_i t + b_i)$ and its cutoff threshold that defines the foot-ground contact flag for the $i$-th foot -- $\mathrm{FOOT\_ON\_GROUND}=\mathbf{1}\{y_i(t) \le c_i\}$. Here, $t \in [1, T]$ is the time step within the cycle. We construct foot contact pattern templates based on the values output by the LLM (e.g., for Baseline 1, we use the random pattern generator; for Baseline 2, we use the sinusoidal functions and the cutoff values) and check if they are correct.

<div style="text-align: center;">
<a href="assets/png/lm_prompt_baselines_v3.png" target="_blank"><img class="b-lazy" src=data:image/jpeg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/lm_prompt_baselines_v3.png" style="display: block; margin: auto; width: 100%;"/></a>
<figcaption>
Baselines prompts. Differences from our prompt are highlighted in blue. The "Gait definition block" is not changed and omitted in the figure. Texts in black are for explanation thus they are not used as input to the LLM. (Click the image to enlarge)<br/>
</figcaption>
</div>


The figure above shows the prompts for the two baselines, where they are based on the our method's prompt with necessary modifications. The following table gives the commands we use in this experiment; commands 1--20 are basic instructions that express explicitly what the robot should do, whereas commands 21--25 test if the interface design allows generalization and pattern composition. We set GPT-4's temperature to $0.5$ to sample diverse responses, and for each approach we submit each command five times. For each submission, we use the top-1 result only for comparisons.

<div style="text-align: center;">
<img class="b-lazy" src=data:image/jpeg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/basic_tests.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption>
Commands for generated pattern template evaluation. We observed the foot contact patterns generated by the LLM after accepting the commands, and compared them against our expectations.<br/>
</figcaption>
</div>

We summarize the results in the figure below. Aggregating over all the commands and test trials, the proposed approach gets significantly ($\sim 50\%$) higher accuracy than the baselines (see the left-most plot in the first row). Despite of having only three conventional examples in the context, the LLM almost always maps the human commands correctly to the expected foot contact patterns. The only exception in the test commands is command 21, where the LLM is correct only one out of the five tests. It mostly fails to generate columns of 0s in the pattern template, but in one interesting case, it appends an extra row of "S: 00...0" to the pattern template, trying to convince us of the existence of the required suspension phase. Baseline 1 gets the second highest accuracy; it achieves high scores on the basic instructions but fails completely for commands 21--25. Considering that this is how we sample patterns and train the controller, these results are somewhat expected. It fails to generate the correct patterns for commands 2--5 because the random pattern generator selects randomly a foot to lift for $G=\mathrm{STAND\_3LEGS}$. Although we could have relaxed the design of Baseline 1 so that it accepted extra parameters for $G$, we didn't have to do so for the proposed approach and it still worked out. Moreover, this design modification has very limited effect and highlights the restrictions imposed by these high-level APIs. Unlike Baseline 1, Baseline 2 should have sufficient freedom in the parameter space to handle all the commands (maybe not command 25), yet its overall accuracy is the worst. Although we performed prompt engineering and constructed the examples carefully in its context for Baseline 2, the LLM has difficulty in understanding the relationship between gaits and the underlying mathematical reasoning.
This limitation again highlights the motivation and demonstrates the importance of the proposed approach.
The experimental results indicate that foot contact pattern is a good interface as it is both straightforward and able to provide more flexibility in the human command space.

<div style="text-align: center;">
<img class="b-lazy" src=data:image/jpeg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/accuracy_comparison.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption>
Accuracy comparison of generated patterns. For each command in the table above, we generate 5 patterns from the LLM and compare them against the expected results. We show the aggregated accuracy over all commands on the left of the first row, followed by the individual results.<br/>
</figcaption>
</div>


### Can we learn to accomplish the main task and realize the contact pattern?

Following <dt-cite key="rudin2022learning"></dt-cite>, we train the locomotion controller with the Proximal policy optimization (PPO) <dt-cite key="schulman2017proximal"></dt-cite> in the IsaacGym simulator <dt-cite key="makoviychuk2021isaac"></dt-cite>. The controller's main task is to track a user specified linear velocity along the robot's heading direction $v_x$, and at the same time, to place the feet correctly to produce the desired foot contact patterns.  The next figure shows the results in simulation.  The commands given to the robot in each trial are shown at the top of the plots. It can be seen from the figure that the controller learns to track the specified velocity (e.g., "slow"/"fast" corresponds to $0.5\frac{m}{s}$/$1.0\frac{m}{s}$ in absolute values) and manages to place the robot's feet correctly to produce foot contact patterns that are close to the desired ones. Furthermore, we successfully transfer the learned controller and deploy it on the physical robot without any fine-tuning.

<div style="text-align: center;">
<a href="assets/png/vel_pattern_analysis.png" target="_blank"><img class="b-lazy" src=data:image/jpeg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/vel_pattern_analysis.png" style="display: block; margin: auto; width: 100%;"/></a>
<figcaption>
Velocity tracking and foot contact pattern realization in simulation. We show the actual linear velocity along the robot's heading direction (first row), the desired foot contact pattern (middle row) and the realized foot contact pattern (last row) from three test trials. The commands given to the robot in each trial are shown at the top of the plots. (Click the image to enlarge)<br/>
</figcaption>
</div>

</br>
<div style="text-align: center;">
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/basic_test1.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/basic_test2.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
</table>
<figcaption style="text-align: left; padding-top: 0;">
Videos that show our A1 following simple/direct instructions.<br/>
</figcaption>
</div>



### Does the proposed approach work with unstructured/vague instructions?


The proposed approach enables both the quadrupedal robot to follow direct and precise commands and unstructured and vague instructions in natural language that facilitates human robot interactions. To demonstrate this, we sent commands in the next table to the robot and observe its reactions. Note that unlike in the previous tests, none of the human expressions here stated explicitly what the robot should have done or what gait it should have used. Based on the subjective evaluation, the observed motions were capable of expressing the desired emotion (e.g., jumping up and down when excited) and presenting the scene accurately (e.g, struggling to move when we told that it had a limping leg), the reactions were mostly consistent with the expectations. This will unlock many robot applications, ranging from scene acting and human companion to more creative tasks in industries and homes.

<div style="text-align: center;">
<img class="b-lazy" src=data:image/jpeg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/extended_tests.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption>
Extended tests. The commands in this test do not tell the robot explicitly what it should do.<br/>
</figcaption>
</div>

</br>
<div style="text-align: center;">
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/extended_test1.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/extended_test2.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/extended_test3.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/extended_test4.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
<td style="width: 100%;border: 0px solid transparent;"><video class="b-lazy" src="assets/mp4/extended_test5.mp4" type="video/mp4" autoplay muted playsinline loop style="width: 100%;" ></video></td></tr>
</table>
<figcaption style="text-align: left; padding-top: 0;">
Videos that show our A1 following unstructured/vague commands.<br/>
</figcaption>
</div>


______


## Conclusions

This paper devised an interactive system for quadrupedal robots that allowed users to craft diverse locomotion behaviours flexibly. The core ideas of the proposed approach include introducing desired foot contact patterns as a new interface between natural language and the low-level controller. During training, these contact patterns are generated by a random generator, and a DRL based method is capable of accomplishing the main task and realizing the desired patterns at the same time. In tests, the contact patterns are translated from human commands in natural language. We show that having contact patterns as the interface is more straightforward and flexible than other design choices. Moreover, the robot is able to follow both direct instructions and commands that do not explicitly state how the robot should react in both simulation and on physical robots. 

### Limitations and Future Work

One limitation of the proposed approach is that domain knowledge and trial-and-error tests are necessary to design the random pattern generator, such that the patterns used for training are feasible. Furthermore, while increasing the variety of the random patterns would essentially increase the locomotion capability of the robot, training on a large set of gaits is hard since it involves the trade-off of sample balancing and data efficiency.

One may train a set of expert policies separately, where each of which specializes in one motion, then use imitation learning to distill the experts to address this problem. Another interesting direction is to introduce multi-modal inputs, such as videos and audios. Foot contact patterns translated from those signals in theory still work with our pipeline and will unlock much more interesting use cases.

______

## Appendix


### More about Random Pattern Generator

The following figure illustrates the steps for the random pattern generator to create a template for $G=\mathrm{PACING}$. To generate a template in general, we first sample $T \in [24, 28]$ (step 1). Since the control frequency is $50$ Hz, this corresponds to a cycle length of $0.48 \sim 0.56$ seconds. We then sample a foot-ground contact length ratio within the cycle $r_\mathrm{contact} \in [0.5, 0.7]$, $Tr_\mathrm{contact}$ therefore gives the number of '1's and $T(1-r_\mathrm{contact})$ the number of '0's in each row (step 2). Proper length scaling and bit shifts of these ones and zeros are necessary to produce feasible foot contact patterns on a real robot (step 3). For $G=\mathrm{BOUND}$, we shorten the foot-ground contact time to $60\%$ of the sampled value (i.e., $r_\mathrm{contact}=0.6r_\mathrm{contact}$), we place the ones at the beginning of the FL and FR rows and shift those in the RL and RR rows by $0.5 Tr_\mathrm{contact}$ bits to the right. We do no scaling for $G=\mathrm{TROT}$. Finally, we shift the '1's to form complete templates (step 4): we place the ones at the beginning in the FL and RR rows and at the end of the FR and RL rows. We keep $r_\mathrm{contact}$ untouched for $G=\mathrm{PACE}$, but shrink the cycle length to half its sampled value (i.e., $T=0.5T$) to make the gait natural and feasible. We place the ones at the beginning in the FL and RL rows and at the end of the FR and RR rows. Finally, for $G \in \{ \mathrm{STAND\_STILL}, \mathrm{STAND\_3LEGS} \}$, we perform no scaling and fill in the pattern template matrix with ones. We randomly sample one row and replace it with zeros if $G=\mathrm{STAND\_3LEGS}$.

<div style="text-align: center;">
<img class="b-lazy" src=data:image/jpeg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/pattern_generator.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption>
How the random pattern generator works.<br/>
</figcaption>
</div>


### Reward Design
Our reward design is based on those in legged gym <dt-cite key="rudin2022learning"></dt-cite>. The total reward consists of 8 weighted reward terms: $J=\sum_{i=1}^{8}w_i r_i$, where $w_i$'s are the weights and $r_i$'s are the rewards. The definition of each reward term and the value of the weights are in the following. We put the purpose of each reward term in the bracket at the beginning of the description.

- [Task Reward] Linear velocity tracking reward. $r_1=e^{-4 \times \large{(} (v_x - \hat{v}_x)^2 + v_y^2 \large{)} }$, where $v_x$ and $\hat{v}_x$ are the current and desired linear velocities along the robot's heading direction, and $v_y$ is the current linear velocity along the lateral direction. All velocities are in the base frame, and $w_1=1$.
- [Task Reward] Angular velocity tracking reward. $r_2=e^{-4 \times \omega_z^2 }$, where $\omega_z$ is the current angular yaw velocity in the base frame and $w_2=-0.5$.
- [Task Reward] Penalty on foot contact pattern violation. $r_3=\frac{1}{4} \sum_{i=1}^4 | c_i - \hat{c}_i |$, where $c_i, \hat{c}_i \in \{0, 1\}$ are the realized and desired foot-ground contact indicators for the $i$-th foot, and $w_3=-1$.
- [Sim-to-Real] Regularization on action rate. $r_4=\sum_{i=1}^{12} (a_t - a_{t-1})^2$ where $a_t$ and $a_{t-1}$ are the controller's output at the current and the previous time steps, and $w_4=-0.005$.
- [Sim-to-Real] Penalty on roll and pitch angular velocities. We encourage the robot's base to be stable during motion and hence $r_5=\omega_x^2 + \omega_y^2$, where $\omega_x$ and $\omega_y$ are the current roll and pitch angular velocities in the base frame. This penalty does not apply to $G=\mathrm{BOUND}$ and $w_5=-0.05$.
 - [Sim-to-Real] Penalty on linear velocity along the z-axis. Similar to the previous term, we use this term to encourage the base stability during motion. $r_6=v_z^2$ where $v_z$ is the current linear velocity along the z-axis in the base frame. This penalty does not apply to $G=\mathrm{BOUND}$ either and $w_6=-2$.
- [Natural Motion] Penalty on body collision. $r_7=\sum_{i=1}^K \mathbf{1}\{F_i > 0.1\}$, where $F_i$ is the contact force on the $i$-th body. In our experiments $K=8$ (i.e., 4 thighs and 4 calves) and $w_7=-1$.
- [Natural Motion] Penalty on deviation from the default pose. $r_8=\sum_{a_t \in \mathrm{hip}} |a_t|$, where $a_t$'s are the actions (i.e., deviation from the default joint position) applied to the hip joints, and $w_8=-0.03$.

### Training Configurations

#### Control
We use PD control to convert positions to torques in our system. The bases value for the 2 gains are $k_p=20$ and $k_d=0.5$. Our control frequency is $50$ Hz.

#### Gait Sampling
We randomly assign a gait $G$ to a robot at environment resets, and also samples it again every 150 steps in simulation. Of the 5 $G$'s, some gaits are harder to learn than others. To avoid the case where the hard-to-learn gaits die out, leaving the controller to learn only on the easier gaits, we restrict the sampling distribution such that the ratio of the 5 $G$'s are always approximately the same.

#### Reinforcement Learning
We use the Proximal policy optimization (PPO) <dt-cite key="schulman2017proximal"></dt-cite> algorithm as our reinforcement learning method to train the controller. In our experiments, PPO trains an actor-critic policy. The architecture of the actor is introduced in the Method section, and the critic has the identical network architecture except that (1) its output size is 1 instead of 12, and (2) it also receives the base velocities in the local frame as its input. We keep all the hyper-parameters the same as in <dt-cite key="rudin2022learning"></dt-cite> and train for 1000 iterations. For safety reasons, we end an episode early if the body height of the robot is lower than 0.25 meters. Training can be done on a singe NVIDIA V100 GPU in approximately 15 minutes.

#### Domain Randomization

During training, we sample noises $\epsilon \sim \mathrm{Unif}$, and add them to the controller's observations. We use PD control to convert positions to torques in our system, and domain randomization is also applied to the 2 gains $k_p$ and $k_d$. The following table gives the components where noises $\epsilon$ were added and their corresponding ranges.


<div style="text-align: center;">
<img class="b-lazy" src=data:image/jpeg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/domain_rand.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption>
Domain randomization settings.<br/>
</figcaption>
</div>

--></dt-article>
<dt-appendix>
<h2>Acknowledgements</h2>
<p>The authors would like to thank <a href="https://scholar.google.com/citations?user=RM2vMNcAAAAJ&amp;hl=en">Tingnan Zhang</a>, <a href="http://www.linkedin.com/in/linda-luu-053273171">Linda Luu</a>, <a href="https://twitter.com/kuanghueilee">Kuang-Huei Lee</a>, <a href="https://vincent.vanhoucke.com/">Vincent Vanhoucke</a> and <a href="https://twitter.com/douglas_eck">Douglas Eck</a> for their valuable discussions and technical support in the experiments.</p>
<p>Any errors here are our own and do not reflect opinions of our proofreaders and colleagues. If you see mistakes or want to suggest changes, feel free to contribute feedback by participating in the <a href="https://github.com/saytap/saytap.github.io/issues">discussion forum</a> for this article.</p>
<p>The experiments in this work were performed on GPU virtual machines provided by <a href="https://cloud.google.com/">Google Cloud Platform</a>.</p>
<div style="text-align: left;">
<img src="assets/icons/robot_dog.svg" alt="Dog walk icon by artist Laymik on Noun Project." style="display: block; margin: auto; width: 4.5%;" align="left"/>&nbsp;&nbsp;Dog walk icon by Laymik from <a href="https://thenounproject.com/browse/icons/term/dog-walk/" target="_blank" title="dog walk Icons">Noun Project</a>.
</div>
<h2 id="citation">Citation</h2>
<!--
<div style="text-align: left;">
<img src="assets/png/gecco_logo.png" alt="GECCO 2020" style="display: block; margin: auto; width: 4.5%;" align="left"/>&nbsp;&nbsp;This work has been presented at <a href="https://gecco-2020.sigevo.org/index.html/HomePage" target="_blank">GECCO 2020</a> as a full paper.
</div>
-->
<p>For attribution in academic contexts, please cite this work as:</p>
<pre class="citation short">Yujin Tang and Wenhao Yu and Jie Tan and Heiga Zen and Aleksandra Faust and Tatsuya Harada,</br>SayTap: Language to Quadrupedal Locomotion, 2023.</pre>
<p>BibTeX citation</p>
<pre class="citation long">@article{saytap2023,
  author = {Yujin Tang and Wenhao Yu and Jie Tan and Heiga Zen and Aleksandra Faust and</br>Tatsuya Harada},
  title  = {SayTap: Language to Quadrupedal Locomotion},
  eprint = {arXiv:2306.07580},
  url    = {https://saytap.github.io},
  note   = "\url{https://saytap.github.io}",
  year   = {2023}
}</pre>
<!--
<pre class="citation long">@inproceedings{attentionagent2020,
  author    = {Yujin Tang and Duong Nguyen and David Ha},
  title     = {Neuroevolution of Self-Interpretable Agents},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  url       = {https://attentionagent.github.io},
  note      = "\url{https://attentionagent.github.io}",
  year      = {2020}
}</pre>
-->
<!--
## Open Source Code

Please see our [repo](https://github.com/google/brain-tokyo-workshop) for details on reproducing the experiments in this work.


## Reuse

Diagrams and text are licensed under Creative Commons Attribution [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) with the [source available on GitHub](https://github.com/attentionagent/attentionagent.github.io), unless noted otherwise. The figures that have been reused from other sources don‚Äôt fall under this license and can be recognized by the citations in their caption.

-->
</dt-appendix>
</dt-appendix>
</body>
<script type="text/bibliography">
  % This file was created with JabRef 2.10.
  % Encoding: UTF-8
  
  @article{makoviychuk2021isaac,
    title={{Isaac Gym}: High performance {GPU}-based physics simulation for robot learning},
    author={Makoviychuk, Viktor and Wawrzyniak, Lukasz and Guo, Yunrong and Lu, Michelle and Storey, Kier and Macklin, Miles and Hoeller, David and Rudin, Nikita and Allshire, Arthur and Handa, Ankur and others},
    journal={arXiv preprint arXiv:2108.10470},
    year={2021}
  }
  
  @inproceedings{rudin2022learning,
    title={Learning to walk in minutes using massively parallel deep reinforcement learning},
    author={Rudin, Nikita and Hoeller, David and Reist, Philipp and Hutter, Marco},
    booktitle={Conference on Robot Learning},
    pages={91--100},
    year={2022},
    organization={PMLR}
  }
  
  @article{schulman2017proximal,
    title={Proximal policy optimization algorithms},
    author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
    journal={arXiv preprint arXiv:1707.06347},
    year={2017}
  }
  
  
  
  @article{tellex2020robots,
    title={Robots that use language},
    author={Tellex, Stefanie and Gopalan, Nakul and Kress-Gazit, Hadas and Matuszek, Cynthia},
    journal={Annual Review of Control, Robotics, and Autonomous Systems},
    volume={3},
    pages={25--55},
    year={2020},
    publisher={Annual Reviews}
  }
  
  @article{lynch2022interactive,
    title={Interactive language: Talking to robots in real time},
    author={Lynch, Corey and Wahid, Ayzaan and Tompson, Jonathan and Ding, Tianli and Betker, James and Baruch, Robert and Armstrong, Travis and Florence, Pete},
    journal={arXiv preprint arXiv:2210.06407},
    year={2022}
  }
  
  @article{mees2022calvin,
    title={Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks},
    author={Mees, Oier and Hermann, Lukas and Rosete-Beas, Erick and Burgard, Wolfram},
    journal={IEEE Robotics and Automation Letters},
    volume={7},
    number={3},
    pages={7327--7334},
    year={2022},
    publisher={IEEE}
  }
  
  @article{brown2020language,
    title={Language models are few-shot learners},
    author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
    journal={Advances in neural information processing systems},
    volume={33},
    pages={1877--1901},
    year={2020}
  }
  
  @article{chowdhery2022palm,
    title={Palm: Scaling language modeling with pathways},
    author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
    journal={arXiv preprint arXiv:2204.02311},
    year={2022}
  }
  
  @article{ahn2022can,
    title={Do as i can, not as i say: Grounding language in robotic affordances},
    author={Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and others},
    journal={arXiv preprint arXiv:2204.01691},
    year={2022}
  }
  
  @article{zeng2022socratic,
    title={Socratic models: Composing zero-shot multimodal reasoning with language},
    author={Zeng, Andy and Wong, Adrian and Welker, Stefan and Choromanski, Krzysztof and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and others},
    journal={arXiv preprint arXiv:2204.00598},
    year={2022}
  }
  
  @article{huang2022inner,
    title={Inner monologue: Embodied reasoning through planning with language models},
    author={Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others},
    journal={arXiv preprint arXiv:2207.05608},
    year={2022}
  }
  
  @article{liang2022code,
    title={Code as policies: Language model programs for embodied control},
    author={Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
    journal={arXiv preprint arXiv:2209.07753},
    year={2022}
  }
  
  @article{kojima2022large,
    title={Large language models are zero-shot reasoners},
    author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
    journal={arXiv preprint arXiv:2205.11916},
    year={2022}
  }
  
  @article{wei2022chain,
    title={Chain of thought prompting elicits reasoning in large language models},
    author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
    journal={arXiv preprint arXiv:2201.11903},
    year={2022}
  }
  
  @article{chen2021evaluating,
    title={Evaluating large language models trained on code},
    author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
    journal={arXiv preprint arXiv:2107.03374},
    year={2021}
  }
  
  @article{kress2008translating,
    title={Translating structured english to robot controllers},
    author={Kress-Gazit, Hadas and Fainekos, Georgios E and Pappas, George J},
    journal={Advanced Robotics},
    volume={22},
    number={12},
    pages={1343--1359},
    year={2008},
    publisher={Taylor \& Francis}
  }
  
  @article{brohan2022rt,
    title={Rt-1: Robotics transformer for real-world control at scale},
    author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
    journal={arXiv preprint arXiv:2212.06817},
    year={2022}
  }
  
  @inproceedings{chai2018language,
    title={Language to Action: Towards Interactive Task Learning with Physical Agents.},
    author={Chai, Joyce Y and Gao, Qiaozi and She, Lanbo and Yang, Shaohua and Saba-Sadiya, Sari and Xu, Guangyue},
    booktitle={IJCAI},
    pages={2--9},
    year={2018}
  }
  
  @inproceedings{howard2014natural,
    title={A natural language planner interface for mobile manipulators},
    author={Howard, Thomas M and Tellex, Stefanie and Roy, Nicholas},
    booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)},
    pages={6652--6659},
    year={2014},
    organization={IEEE}
  }
  
  @article{stepputtis2020language,
    title={Language-conditioned imitation learning for robot manipulation tasks},
    author={Stepputtis, Simon and Campbell, Joseph and Phielipp, Mariano and Lee, Stefan and Baral, Chitta and Ben Amor, Heni},
    journal={Advances in Neural Information Processing Systems},
    volume={33},
    pages={13139--13150},
    year={2020}
  }
  
  @article{mees2022matters,
    title={What matters in language conditioned robotic imitation learning over unstructured data},
    author={Mees, Oier and Hermann, Lukas and Burgard, Wolfram},
    journal={IEEE Robotics and Automation Letters},
    volume={7},
    number={4},
    pages={11205--11212},
    year={2022},
    publisher={IEEE}
  }
  
  @inproceedings{NEURIPS2022_b1efde53,
   author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
   booktitle = {Advances in Neural Information Processing Systems},
   editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
   pages = {27730--27744},
   publisher = {Curran Associates, Inc.},
   title = {Training language models to follow instructions with human feedback},
   url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
   volume = {35},
   year = {2022}
  }
  
  @article{singh2022progprompt,
    title={Progprompt: Generating situated robot task plans using large language models},
    author={Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
    journal={arXiv preprint arXiv:2209.11302},
    year={2022}
  }
  
  @article{vemprala2023chatgpt,
    title={Chatgpt for robotics: Design principles and model abilities},
    author={Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
    journal={Microsoft Autonomous Systems and Robotics Research},
    year={2023}
  }
  
  
  @article{lin2023text2motion,
    title={Text2motion: From natural language instructions to feasible plans},
    author={Lin, Kevin and Agia, Christopher and Migimatsu, Toki and Pavone, Marco and Bohg, Jeannette},
    journal={arXiv preprint arXiv:2303.12153},
    year={2023}
  }
  
  @inproceedings{shridhar2022cliport,
    title={Cliport: What and where pathways for robotic manipulation},
    author={Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
    booktitle={Conference on Robot Learning},
    pages={894--906},
    year={2022},
    organization={PMLR}
  }
  
  @article{bucker2022latte,
    title={LaTTe: Language Trajectory TransformEr},
    author={Bucker, Arthur and Figueredo, Luis and Haddadin, Sami and Kapoor, Ashish and Ma, Shuang and Bonatti, Rogerio},
    journal={arXiv preprint arXiv:2208.02918},
    year={2022}
  }
  
  @article{raibert1990trotting,
    title={Trotting, pacing and bounding by a quadruped robot},
    author={Raibert, Marc H},
    journal={Journal of biomechanics},
    volume={23},
    pages={79--98},
    year={1990},
    publisher={Elsevier}
  }
  
  @inproceedings{di2018dynamic,
    title={Dynamic locomotion in the mit cheetah 3 through convex model-predictive control},
    author={Di Carlo, Jared and Wensing, Patrick M and Katz, Benjamin and Bledt, Gerardo and Kim, Sangbae},
    booktitle={2018 IEEE/RSJ international conference on intelligent robots and systems (IROS)},
    pages={1--9},
    year={2018},
    organization={IEEE}
  }
  
  @inproceedings{eckert2015comparing,
    title={Comparing the effect of different spine and leg designs for a small bounding quadruped robot},
    author={Eckert, Peter and Spr{\"o}witz, Alexander and Witte, Hartmut and Ijspeert, Auke Jan},
    booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)},
    pages={3128--3133},
    year={2015},
    organization={IEEE}
  }
  
  @article{marhefka2003intelligent,
    title={Intelligent control of quadruped gallops},
    author={Marhefka, Duane W and Orin, David E and Schmiedeler, James P and Waldron, Kenneth J},
    journal={IEEE/ASME Transactions On Mechatronics},
    volume={8},
    number={4},
    pages={446--456},
    year={2003},
    publisher={IEEE}
  }
  
  @inproceedings{grandia2019feedback,
    title={Feedback {MPC} for torque-controlled legged robots},
    author={Grandia, Ruben and Farshidian, Farbod and Ranftl, Ren{\'e} and Hutter, Marco},
    booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
    pages={4730--4737},
    year={2019},
    organization={IEEE}
  }
  
  @article{li2022versatile,
    title={Versatile Real-Time Motion Synthesis via Kino-Dynamic MPC with Hybrid-Systems DDP},
    author={Li, He and Zhang, Tingnan and Yu, Wenhao and Wensing, Patrick M},
    journal={arXiv preprint arXiv:2209.14138},
    year={2022}
  }
  
  @inproceedings{villarreal2020mpc,
    title={Mpc-based controller with terrain insight for dynamic legged locomotion},
    author={Villarreal, Octavio and Barasuol, Victor and Wensing, Patrick M and Caldwell, Darwin G and Semini, Claudio},
    booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)},
    pages={2436--2442},
    year={2020},
    organization={IEEE}
  }
  
  @article{winkler18,
    author    = {Winkler, Alexander W and Bellicoso, Dario C and 
                 Hutter, Marco and Buchli, Jonas},
    title     = {Gait and Trajectory Optimization for Legged Systems 
                 through Phase-based End-Effector Parameterization},
    journal   = {IEEE Robotics and Automation Letters (RA-L)},
    year      = {2018},
    month     = {July},
    pages     = {1560-1567},
    volume    = {3},
    doi       = {10.1109/LRA.2018.2798285},
  }
  
  @inproceedings{siekmann2021sim,
    title={Sim-to-real learning of all common bipedal gaits via periodic reward composition},
    author={Siekmann, Jonah and Godse, Yesh and Fern, Alan and Hurst, Jonathan},
    booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
    pages={7309--7315},
    year={2021},
    organization={IEEE}
  }
  
  @inproceedings{yang2020data,
    title={Data efficient reinforcement learning for legged robots},
    author={Yang, Yuxiang and Caluwaerts, Ken and Iscen, Atil and Zhang, Tingnan and Tan, Jie and Sindhwani, Vikas},
    booktitle={Conference on Robot Learning},
    pages={1--10},
    year={2020},
    organization={PMLR}
  }
  
  @article{margolis2021learning,
    title={Learning to jump from pixels},
    author={Margolis, Gabriel B and Chen, Tao and Paigwar, Kartik and Fu, Xiang and Kim, Donghyun and Kim, Sangbae and Agrawal, Pulkit},
    journal={arXiv preprint arXiv:2110.15344},
    year={2021}
  }
  
  @inproceedings{iscen2018policies,
    title={Policies modulating trajectory generators},
    author={Iscen, Atil and Caluwaerts, Ken and Tan, Jie and Zhang, Tingnan and Coumans, Erwin and Sindhwani, Vikas and Vanhoucke, Vincent},
    booktitle={Conference on Robot Learning},
    pages={916--926},
    year={2018},
    organization={PMLR}
  }
  
  
  @article{hwangbo2019learning,
    title={Learning agile and dynamic motor skills for legged robots},
    author={Hwangbo, Jemin and Lee, Joonho and Dosovitskiy, Alexey and Bellicoso, Dario and Tsounis, Vassilios and Koltun, Vladlen and Hutter, Marco},
    journal={Science Robotics},
    volume={4},
    number={26},
    pages={eaau5872},
    year={2019},
    publisher={American Association for the Advancement of Science}
  }
  
  @inproceedings{da2021learning,
    title={Learning a contact-adaptive controller for robust, efficient legged locomotion},
    author={Da, Xingye and Xie, Zhaoming and Hoeller, David and Boots, Byron and Anandkumar, Anima and Zhu, Yuke and Babich, Buck and Garg, Animesh},
    booktitle={Conference on Robot Learning},
    pages={883--894},
    year={2021},
    organization={PMLR}
  }
  
  @article{openai2023gpt,
    title={{GPT-4} technical report},
    author={OpenAI},
    journal={arXiv},
    year={2023}
  }
  
  @misc{unitree,
  author={{Unitree Robotics}},
  note={\url{https://unitreerobotics.net/}},
  year = {2023}
  }
  
  @inproceedings{borenstein1997guidecane,
    title={The guidecane-a computerized travel aid for the active guidance of blind pedestrians},
    author={Borenstein, Johann and Ulrich, Iwan},
    booktitle={Proceedings of International Conference on Robotics and Automation},
    volume={2},
    pages={1283--1288},
    year={1997},
    organization={IEEE}
  }
  
  @inproceedings{chuang2018deep,
    title={Deep trail-following robotic guide dog in pedestrian environments for people who are blind and visually impaired-learning from virtual and real worlds},
    author={Chuang, Tzu-Kuan and Lin, Ni-Ching and Chen, Jih-Shi and Hung, Chen-Hao and Huang, Yi-Wei and Teng, Chunchih and Huang, Haikun and Yu, Lap-Fai and Giarr{\'e}, Laura and Wang, Hsueh-Cheng},
    booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
    pages={5849--5855},
    year={2018},
    organization={IEEE}
  }
  
  @article{mehrizi2021quadrupedal,
    title={Quadrupedal Robotic Guide Dog with Vocal Human-Robot Interaction},
    author={Mehrizi, Kavan},
    journal={arXiv preprint arXiv:2111.03718},
    year={2021}
  }
  
  @inproceedings{margolis2023walk,
    title={Walk these ways: Tuning robot control for generalization with multiplicity of behavior},
    author={Margolis, Gabriel B and Agrawal, Pulkit},
    booktitle={Conference on Robot Learning},
    pages={22--31},
    year={2023},
    organization={PMLR}
  }
  
  @inproceedings{jiang2021dash,
    title={Dash: Modularized human manipulation simulation with vision and language for embodied ai},
    author={Jiang, Yifeng and Guo, Michelle and Li, Jiangshan and Exarchos, Ioannis and Wu, Jiajun and Liu, C Karen},
    booktitle={The ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
    pages={1--12},
    year={2021}
  }
</script>
<script src="lib/blazy.js"></script>
<script>
  // blazy code
  var bLazy = new Blazy({
    success: function(){
      updateCounter();
    }
  });

  // not needed, only here to illustrate amount of loaded images
  var imageLoaded = 0;

  function updateCounter() {
    imageLoaded++;
    console.log("blazy image loaded: "+imageLoaded);
  }
</script>